{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports and definition of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import datetime \n",
    "\n",
    "input_path = \"dataset/raw\"\n",
    "output_path =\"dataset/preprocessed/\"\n",
    "\n",
    "look_back_interval=10\n",
    "all_weather = False\n",
    "save_intermediary_files = True\n",
    "\n",
    "output = pathlib.Path(output_path)\n",
    "output.mkdir(parents=True, exist_ok=True)\n",
    "input = pathlib.Path(input_path)\n",
    "\n",
    "kpi_file= input/\"rl-kpis.tsv\"\n",
    "distances_file = input/\"distances.tsv\"\n",
    "met_forecast_file = input/\"met-forecast.tsv\"\n",
    "rl_sites_file = input/\"rl-sites.tsv\"\n",
    "met_stations_file = input/\"met-stations.tsv\"\n",
    "met_real_file = input/\"met-real.tsv\"\n",
    "\n",
    "other = [\"precipitation_min\", \"precipitation_max\", \"precipitation_std\", \"precipitation_mean\"]\n",
    "\n",
    "removable_features = [\"unavail_second\", \"severaly_error_second\", \"error_second\", \"bbe\", \"avail_time\", \"mw_connection_no\", \"precipitation_coeff_max\"]\n",
    "\n",
    "ordinal_enconder = OrdinalEncoder(categories=[['clear sky', 'hot day', 'scattered clouds', 'few clouds', 'overcast clouds','foggy', 'windy', 'misty', 'light rain', \n",
    "                                              'light rain showers', 'light intensity shower rain', 'light snow','snow', 'sleet',\n",
    "                                                'rain', 'heavy rain showers','heavy rain', \n",
    "                                              'thunderstorm with heavy rain', 'heavy thunderstorm with rain showers']],\n",
    "                                  \n",
    "                                  handle_unknown='use_encoded_value',\n",
    "                                  unknown_value=np.nan\n",
    "                                  )\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find K closest stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_closest_stations(rl_sites_file, distances_file, met_stations_file, k=3):\n",
    "    rl_sites_df = pd.read_csv(rl_sites_file, sep=\"\\t\", index_col=0)\n",
    "    distances_df = pd.read_csv(distances_file, sep=\"\\t\", index_col=0)\n",
    "    met_stations_df = pd.read_csv(met_stations_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "    rl_stations = rl_sites_df[\"site_id\"].unique()\n",
    "    met_stations = met_stations_df[\"station_no\"].unique()\n",
    "\n",
    "    distances_df = distances_df.loc[met_stations, rl_stations]\n",
    "\n",
    "    closest_stations = dict()\n",
    "    for rl_station in rl_stations:\n",
    "        closest_stations[rl_station] = set(distances_df.nsmallest(k, [rl_station]).index.tolist())\n",
    "\n",
    "    return closest_stations\n",
    "\n",
    "#select most frequent option\n",
    "def select_frequent(series):\n",
    "    frequent = pd.Series.mode(series)\n",
    "    if len(frequent) > 1:\n",
    "        return frequent[1]\n",
    "    elif len(frequent) > 0:\n",
    "        return frequent[0]\n",
    "    \n",
    "    return np.NaN\n",
    "\n",
    "def timeseries_processing(time_sentitive_dataset, time_sensitive_features, identifiers, labels, output, df_type = \"train\", full=True):\n",
    "    ordered_features = []\n",
    "    for i in range(1-look_back_interval, 0, 1):\n",
    "        for feature in time_sensitive_features:\n",
    "            ordered_features.append(f\"T{i}_{feature}\")\n",
    "    \n",
    "    ordered_features += time_sensitive_features\n",
    "    if full:\n",
    "        for feature in time_sensitive_features:\n",
    "            historical_sen_dataset = time_sentitive_dataset.loc[:, identifiers]\n",
    "            for i in range(-1,  -look_back_interval, -1):\n",
    "                historical_sen_dataset[f\"T{i}\"] = historical_sen_dataset[\"datetime\"] + pd.DateOffset(days=i)\n",
    "\n",
    "            feature_view = time_sentitive_dataset[identifiers + [feature]]\n",
    "            for i in range(-1,  -look_back_interval, -1):\n",
    "                target_day_column_name = f\"T{i}\"\n",
    "\n",
    "                historical_sen_dataset = historical_sen_dataset.merge(feature_view, \n",
    "                        how = \"left\", \n",
    "                        left_on = (\"site_id\", \"mlid\", target_day_column_name),\n",
    "                        right_on = identifiers,\n",
    "                        suffixes = (\"\", \"_y\")\n",
    "                )\n",
    "                historical_sen_dataset.rename(columns={ feature: f\"{target_day_column_name}_{feature}\"}, inplace=True)\n",
    "\n",
    "            historical_sen_dataset.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "            historical_sen_dataset.drop(columns=[f\"T{i}\" for i in range(-1,  -look_back_interval, -1)], inplace=True)\n",
    "\n",
    "            time_sentitive_dataset = time_sentitive_dataset.merge(historical_sen_dataset, \n",
    "                        how=\"left\", \n",
    "                        on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "        time_sentitive_dataset = time_sentitive_dataset.dropna()\n",
    "        time_sentitive_dataset[labels] = time_sentitive_dataset[labels].astype(int)\n",
    "\n",
    "        time_sentitive_dataset = time_sentitive_dataset[identifiers + ordered_features + labels]\n",
    "        if df_type != \"train\":\n",
    "            if len(time_sentitive_dataset) > 1:\n",
    "                np.savetxt(output/f\"x_{df_type}.csv\", time_sentitive_dataset[ordered_features].values, delimiter=\",\", fmt=\"%5.2f\")\n",
    "                np.savetxt(output/f\"y_{df_type}.csv\", time_sentitive_dataset[[\"1-day-predict\"]].values, delimiter=\",\", fmt=\"%d\")\n",
    "        else:\n",
    "            rl_mlid_combos =  (time_sentitive_dataset[\"site_id\"] + \"%\" + time_sentitive_dataset[\"mlid\"]).unique()\n",
    "\n",
    "            for rl_mlid in rl_mlid_combos:\n",
    "                site_id, mlid = rl_mlid.split(\"%\")\n",
    "                rl_mlid_df = time_sentitive_dataset.loc[(time_sentitive_dataset[\"site_id\"] == site_id) & (time_sentitive_dataset[\"mlid\"] == mlid)]\n",
    "                if len(rl_mlid_df) > 1:\n",
    "                        site_folder = output / site_id\n",
    "                        site_folder.mkdir(parents=True, exist_ok=True)\n",
    "                        rl_mlid_df.to_csv(site_folder/f\"{mlid}_time_sensitive_features.csv\", index=None)\n",
    "    else:\n",
    "        if df_type != \"train\":\n",
    "            for feature in time_sensitive_features:\n",
    "                historical_sen_dataset = time_sentitive_dataset.loc[:, identifiers]\n",
    "                for i in range(-1,  -look_back_interval, -1):\n",
    "                    historical_sen_dataset[f\"T{i}\"] = historical_sen_dataset[\"datetime\"] + pd.DateOffset(days=i)\n",
    "\n",
    "                feature_view = time_sentitive_dataset[identifiers + [feature]]\n",
    "                for i in range(-1,  -look_back_interval, -1):\n",
    "                    target_day_column_name = f\"T{i}\"\n",
    "\n",
    "                    historical_sen_dataset = historical_sen_dataset.merge(feature_view, \n",
    "                            how = \"left\", \n",
    "                            left_on = (\"site_id\", \"mlid\", target_day_column_name),\n",
    "                            right_on = identifiers,\n",
    "                            suffixes = (\"\", \"_y\")\n",
    "                    )\n",
    "                    historical_sen_dataset.rename(columns={ feature: f\"{target_day_column_name}_{feature}\"}, inplace=True)\n",
    "\n",
    "                historical_sen_dataset.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "                historical_sen_dataset.drop(columns=[f\"T{i}\" for i in range(-1,  -look_back_interval, -1)], inplace=True)\n",
    "\n",
    "                time_sentitive_dataset = time_sentitive_dataset.merge(historical_sen_dataset, \n",
    "                            how=\"left\", \n",
    "                            on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "            time_sentitive_dataset = time_sentitive_dataset.dropna()\n",
    "            time_sentitive_dataset[labels] = time_sentitive_dataset[labels].astype(int)\n",
    "\n",
    "            time_sentitive_dataset = time_sentitive_dataset[identifiers + ordered_features + labels]\n",
    "            if len(time_sentitive_dataset) > 1:\n",
    "                np.savetxt(output/f\"x_{df_type}.csv\", time_sentitive_dataset[ordered_features].values, delimiter=\",\", fmt=\"%5.2f\")\n",
    "                np.savetxt(output/f\"y_{df_type}.csv\", time_sentitive_dataset[[\"1-day-predict\"]].values, delimiter=\",\", fmt=\"%d\")\n",
    "        else:\n",
    "            output = output/\"train_data\"\n",
    "            output.mkdir(parents=True, exist_ok=True)\n",
    "            rl_mlid_combos =  (time_sentitive_dataset[\"site_id\"] + \"%\" + time_sentitive_dataset[\"mlid\"]).unique()\n",
    "\n",
    "            for rl_mlid in rl_mlid_combos:\n",
    "                site_id, mlid = rl_mlid.split(\"%\")\n",
    "                rl_mlid_df = time_sentitive_dataset.loc[(time_sentitive_dataset[\"site_id\"] == site_id) & (time_sentitive_dataset[\"mlid\"] == mlid)]\n",
    "\n",
    "                for feature in time_sensitive_features:\n",
    "                    historical_sen_dataset = rl_mlid_df.loc[:, identifiers]\n",
    "                    for i in range(-1,  -look_back_interval, -1):\n",
    "                        historical_sen_dataset[f\"T{i}\"] = historical_sen_dataset[\"datetime\"] + pd.DateOffset(days=i)\n",
    "\n",
    "                    feature_view = rl_mlid_df[identifiers + [feature]]\n",
    "                    for i in range(-1,  -look_back_interval, -1):\n",
    "                        target_day_column_name = f\"T{i}\"\n",
    "\n",
    "                        historical_sen_dataset = historical_sen_dataset.merge(feature_view, \n",
    "                                how = \"left\", \n",
    "                                left_on = (\"site_id\", \"mlid\", target_day_column_name),\n",
    "                                right_on = identifiers,\n",
    "                                suffixes = (\"\", \"_y\")\n",
    "                        )\n",
    "                        historical_sen_dataset.rename(columns={ feature: f\"{target_day_column_name}_{feature}\"}, inplace=True)\n",
    "\n",
    "                    historical_sen_dataset.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "                    historical_sen_dataset.drop(columns=[f\"T{i}\" for i in range(-1,  -look_back_interval, -1)], inplace=True)\n",
    "\n",
    "                    rl_mlid_df = rl_mlid_df.merge(historical_sen_dataset, \n",
    "                                how=\"left\", \n",
    "                                on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "                rl_mlid_df = rl_mlid_df.dropna()\n",
    "                rl_mlid_df[labels] = rl_mlid_df[labels].astype(int)\n",
    "\n",
    "                rl_mlid_df = rl_mlid_df[identifiers + ordered_features + labels]\n",
    "                \n",
    "                if len(rl_mlid_df) > 1:\n",
    "                    site_folder = output / site_id\n",
    "                    site_folder.mkdir(parents=True, exist_ok=True)\n",
    "                    rl_mlid_df.to_csv(site_folder/f\"{mlid}_time_sensitive_features.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading forecasting data\n",
    "met_forecast_df = pd.read_csv(met_forecast_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "met_forecast_df[\"datetime\"] = pd.to_datetime(met_forecast_df[\"datetime\"])\n",
    "\n",
    "# Filtering the reports to only include the morning report, and removing that column afterwards. (usually there are morning and afternoon reports)\n",
    "met_forecast_df = met_forecast_df.groupby(by=[\"station_no\", \"datetime\"], group_keys=False).agg(select_frequent)\n",
    "met_forecast_df.reset_index(level=[\"station_no\", \"datetime\"], inplace=True)\n",
    "met_forecast_df.drop(columns=[\"report_time\"], inplace=True)\n",
    "\n",
    "#Filtering between 5 days of forecast or just day 1\n",
    "if not all_weather:\n",
    "    columns = [column for column in met_forecast_df.columns if \"day1\" not in column and column not in [\"station_no\", \"datetime\"]]\n",
    "    met_forecast_df.drop(columns=columns, inplace=True)\n",
    "\n",
    "## Transforming weather features in ordinal encoding vectors\n",
    "if all_weather:\n",
    "    weather_features = [f\"weather_day{i}\" for i in range(1,6)]\n",
    "else:\n",
    "    weather_features = [\"weather_day1\"]\n",
    "\n",
    "for weather_feature in weather_features:\n",
    "    met_forecast_df[weather_feature] = ordinal_enconder.fit_transform(met_forecast_df[weather_feature].to_numpy().reshape(-1,1).tolist()) \n",
    "\n",
    "# Loading the real data and removing the hour of when it was collected\n",
    "met_real_df = pd.read_csv(met_real_file, sep=\"\\t\", index_col=0)\n",
    "met_real_df.drop(columns=[\"datetime\", \"measured_hour\"], inplace=True)\n",
    "\n",
    "met_real_df[\"measured_date\"] = pd.to_datetime(met_real_df[\"measured_date\"])\n",
    "met_real_df.rename(columns={\"measured_date\":\"datetime\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading KPI file & finding the closest stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the closest met station to each radio link one.\n",
    "closest_stations = finding_closest_stations(rl_sites_file, distances_file, met_stations_file)\n",
    "\n",
    "#List of columns that uniquelly identify an entry in the kpi_df\n",
    "identifiers = [\"site_id\", \"mlid\", \"datetime\"]\n",
    "\n",
    "kpi_df = pd.read_csv(kpi_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "#Transform the datetime column to the correct format\n",
    "kpi_df[\"datetime\"] = pd.to_datetime(kpi_df[\"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating real and forecast weather for each site_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_real_agg_df = None\n",
    "#Adding closest station to each entry according to the id of the radio link\n",
    "for site_id in kpi_df[\"site_id\"].unique():\n",
    "    temp_df = met_real_df[met_real_df[\"station_no\"].isin(closest_stations[site_id])]\\\n",
    "                    .drop(columns=[\"station_no\"])   \\\n",
    "                    .groupby(by=[\"datetime\"], group_keys=False).agg(['min', 'max', 'mean', 'std'])\n",
    "    \n",
    "    temp_df.reset_index(level=[\"datetime\"], inplace=True)\n",
    "    temp_df[\"site_id\"] = site_id\n",
    "\n",
    "    if type(met_real_agg_df) == pd.DataFrame:\n",
    "        met_real_agg_df = pd.concat([met_real_agg_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        met_real_agg_df = temp_df\n",
    "\n",
    "#Transforming multi-level columns to single level\n",
    "met_real_agg_df.columns = [\"_\".join(x)  if x[1] != '' else x[0] for x in met_real_agg_df.columns]\n",
    "\n",
    "#Mergin k closest stations to each entry according to the id of the radio link\n",
    "\n",
    "met_forecast_agg_df = None\n",
    "\n",
    "for site_id in kpi_df[\"site_id\"].unique():\n",
    "    filtered_df = met_forecast_df[met_forecast_df[\"station_no\"].isin(closest_stations[site_id])]\\\n",
    "                    .drop(columns=[\"station_no\"])\n",
    "                    \n",
    "    temp_df = filtered_df.drop(columns=[\"weather_day1\"]).groupby(by=[\"datetime\"], group_keys=False).mean()\n",
    "\n",
    "    temp_df[\"weather_day1\"] = filtered_df[[\"datetime\", \"weather_day1\"]].groupby(by=[\"datetime\"], group_keys=False) \\\n",
    "                    .agg(select_frequent)[\"weather_day1\"]    \n",
    "    \n",
    "    temp_df.reset_index(level=[\"datetime\"], inplace=True)\n",
    "    temp_df[\"site_id\"] = site_id\n",
    "\n",
    "    if type(met_forecast_agg_df) == pd.DataFrame:\n",
    "        met_forecast_agg_df = pd.concat([met_forecast_agg_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        met_forecast_agg_df = temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging KPI df with met forecast and real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging kpis with forecast data\n",
    "kpi_df = kpi_df.merge(met_forecast_agg_df, on=[\"datetime\", \"site_id\"])\n",
    "kpi_df = kpi_df.merge(met_real_agg_df, on=[\"datetime\", \"site_id\"])\n",
    "\n",
    "#Remove unnecessary columns\n",
    "kpi_df.drop(columns=removable_features, inplace=True)\n",
    "\n",
    "if \"scalibility_score\" in kpi_df.columns:\n",
    "    kpi_df.drop(columns=[\"scalibility_score\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the labels for each entry 1-day after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = kpi_df.loc[:, identifiers]\n",
    "\n",
    "df_labels[\"T+1\"] = df_labels[\"datetime\"] + pd.DateOffset(days=1)\n",
    "\n",
    "df_labels_view = kpi_df[identifiers + [\"rlf\"]]\n",
    "\n",
    "df_labels = df_labels.merge(df_labels_view, \n",
    "            how = \"left\", \n",
    "            left_on = (\"site_id\", \"mlid\", \"T+1\"),\n",
    "            right_on = identifiers,\n",
    "            suffixes = (\"\", \"_y\")\n",
    ")\n",
    "df_labels.rename(columns={\"rlf\": \"T+1_rlf\"}, inplace=True)\n",
    "\n",
    "df_labels.drop(columns=[\"datetime_y\"], inplace=True)\n",
    "\n",
    "df_labels[\"1-day-predict\"] = df_labels[\"T+1_rlf\"]\n",
    "\n",
    "df_labels = df_labels[[\"datetime\", \"site_id\", \"mlid\", \"1-day-predict\"]]\n",
    "\n",
    "kpi_df = kpi_df.merge(df_labels, \n",
    "                        how=\"left\", \n",
    "                        on=[\"datetime\", \"site_id\", \"mlid\"])\n",
    "\n",
    "\n",
    "static_features = [\"card_type\", \"freq_band\", \"type\", \"tip\", \"adaptive_modulation\", \"freq_band\", \"modulation\"]\n",
    "labels = [\"rlf\", \"1-day-predict\"]\n",
    "\n",
    "time_sensitive_features = [feature for feature in kpi_df.columns if feature not in static_features and feature not in labels and feature not in identifiers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating static / time dependent features and creating the static dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sentitive_dataset = kpi_df.loc[:, identifiers + time_sensitive_features + labels]\n",
    "\n",
    "static_dataset = kpi_df.loc[:, identifiers + static_features + labels]\n",
    "static_dataset = static_dataset.dropna()\n",
    "\n",
    "one_hot_encoder.fit(static_dataset[static_features])\n",
    "\n",
    "static_dataset = pd.concat([static_dataset,\n",
    "                            pd.DataFrame(one_hot_encoder.transform(static_dataset[static_features]).toarray(),\n",
    "                                                         columns=one_hot_encoder.get_feature_names_out())\n",
    "                            ],\n",
    "                        axis=1\n",
    "                        )\n",
    "\n",
    "static_dataset.drop(columns=static_features, inplace=True)\n",
    "\n",
    "if save_intermediary_files:\n",
    "    static_dataset.to_csv(output/\"preprocessed_static_features.csv\", index=None)\n",
    "    time_sentitive_dataset.to_csv(output/\"preprocessed_timeseries.csv\", index=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing between train/cv/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(time_sentitive_dataset)\n",
    "train_df = time_sentitive_dataset.loc[time_sentitive_dataset[\"datetime\"] < datetime.datetime(2019, 9,1)]\n",
    "\n",
    "cv_df = time_sentitive_dataset.loc[(time_sentitive_dataset[\"datetime\"] >= datetime.datetime(2019, 9,1)) \\\n",
    "                                   & (time_sentitive_dataset[\"datetime\"] < datetime.datetime(2020, 1,1) )]\n",
    "\n",
    "test_df = time_sentitive_dataset.loc[(time_sentitive_dataset[\"datetime\"] >= datetime.datetime(2020, 1,1) )\\\n",
    "                                     & (time_sentitive_dataset[\"datetime\"] < datetime.datetime(2020, 6,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13712/3198831641.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_clean[time_sensitive_features] = (train_df_clean[time_sensitive_features] - mean_values) /std_values\n",
      "/tmp/ipykernel_13712/3198831641.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cv_df_clean[time_sensitive_features] = (cv_df_clean[time_sensitive_features] - mean_values) / std_values\n",
      "/tmp/ipykernel_13712/3198831641.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df_clean[time_sensitive_features] = (test_df_clean[time_sensitive_features] - mean_values) / std_values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_values = train_df[time_sensitive_features].mean()\n",
    "std_values = train_df[time_sensitive_features].std()\n",
    "\n",
    "removable = (np.abs((train_df[time_sensitive_features] - mean_values) / std_values) < 3).all(axis=1)\n",
    "if not removable.all():\n",
    "    train_df_clean = train_df[removable]\n",
    "\n",
    "removable = (np.abs((cv_df[time_sensitive_features] - mean_values) / std_values) < 3).all(axis=1)\n",
    "if not removable.all():\n",
    "    cv_df_clean = cv_df[removable]\n",
    "\n",
    "removable = (np.abs((test_df[time_sensitive_features] - mean_values) / std_values) < 3).all(axis=1)\n",
    "if not removable.all():\n",
    "    test_df_clean = test_df[removable]\n",
    "\n",
    "mean_values = train_df[time_sensitive_features].mean()\n",
    "std_values = train_df[time_sensitive_features].std()\n",
    "\n",
    "train_df_clean[time_sensitive_features] = (train_df_clean[time_sensitive_features] - mean_values) /std_values\n",
    "cv_df_clean[time_sensitive_features] = (cv_df_clean[time_sensitive_features] - mean_values) / std_values\n",
    "test_df_clean[time_sensitive_features] = (test_df_clean[time_sensitive_features] - mean_values) / std_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "(447687, 61) (613858, 61)\n",
      "391 508\n",
      "Cross val\n",
      "(266863, 61) (335020, 61)\n",
      "112 139\n",
      "Test\n",
      "(282252, 61) (405948, 61)\n",
      "49 72\n"
     ]
    }
   ],
   "source": [
    "#timeseries_processing(cv_df_clean, time_sensitive_features, identifiers, labels, output, df_type = \"cv\")\n",
    "print(\"Train\")\n",
    "print(train_df_clean.shape, train_df.shape)\n",
    "print(train_df_clean[\"1-day-predict\"].sum(), train_df[\"1-day-predict\"].sum())\n",
    "\n",
    "print(\"Cross val\")\n",
    "print(cv_df_clean.shape, cv_df.shape)\n",
    "print(cv_df_clean[\"1-day-predict\"].sum(), cv_df[\"1-day-predict\"].sum())\n",
    "\n",
    "print(\"Test\")\n",
    "print(test_df_clean.shape, test_df.shape)\n",
    "print(test_df_clean[\"1-day-predict\"].sum(), test_df[\"1-day-predict\"].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1 = train_df[time_sensitive_features].quantile(0.25)\n",
    "#q3 = train_df[time_sensitive_features].quantile(0.75)\n",
    "#iqr =  q3 - q1\n",
    "#max_value = q3 + iqr*1.5\n",
    "#min_value = q1 - iqr*1.5\n",
    "#\n",
    "#train_df = train_df[((train_df[time_sensitive_features] > min_value) & (train_df[time_sensitive_features] < max_value)).all(axis=1)]\n",
    "#cv_df = cv_df[((cv_df[time_sensitive_features] > min_value) & (cv_df[time_sensitive_features] < max_value)).all(axis=1)]\n",
    "#test_df = test_df[((test_df[time_sensitive_features] > min_value) & (test_df[time_sensitive_features] < max_value)).all(axis=1)]\n",
    "\n",
    "#mean_values = train_df[time_sensitive_features].mean()\n",
    "#std_values = train_df[time_sensitive_features].std()\n",
    "#\n",
    "#train_df.loc[:, time_sensitive_features] = (train_df[time_sensitive_features] - mean_values) / std_values\n",
    "#\n",
    "#cv_df.loc[:, time_sensitive_features] = (cv_df[time_sensitive_features] - mean_values) / std_values\n",
    "#\n",
    "#test_df.loc[:, time_sensitive_features] = (test_df[time_sensitive_features] - mean_values) / std_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating timeseries vectors for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'rl_mlid_combos' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m timeseries_processing(cv_df_clean, time_sensitive_features, identifiers, labels, output, df_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m timeseries_processing(test_df_clean, time_sensitive_features, identifiers, labels, output, df_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m timeseries_processing(train_df_clean, time_sensitive_features, identifiers, labels, output, df_type \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb Cell 24\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m         np\u001b[39m.\u001b[39msavetxt(output\u001b[39m/\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_\u001b[39m\u001b[39m{\u001b[39;00mdf_type\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m, time_sentitive_dataset[[\u001b[39m\"\u001b[39m\u001b[39m1-day-predict\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39mvalues, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, fmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mfor\u001b[39;00m rl_mlid \u001b[39min\u001b[39;00m rl_mlid_combos:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         site_id, mlid \u001b[39m=\u001b[39m rl_mlid\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Desktop/stuff/uni/artigos/fl_costs/fl_costs_extended/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         rl_mlid_df \u001b[39m=\u001b[39m time_sentitive_dataset\u001b[39m.\u001b[39mloc[(time_sentitive_dataset[\u001b[39m\"\u001b[39m\u001b[39msite_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m site_id) \u001b[39m&\u001b[39m (time_sentitive_dataset[\u001b[39m\"\u001b[39m\u001b[39mmlid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m mlid)]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'rl_mlid_combos' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "timeseries_processing(cv_df_clean, time_sensitive_features, identifiers, labels, output, df_type = \"cv\")\n",
    "timeseries_processing(test_df_clean, time_sensitive_features, identifiers, labels, output, df_type = \"test\")\n",
    "timeseries_processing(train_df_clean, time_sensitive_features, identifiers, labels, output, df_type = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(train_df, cv_df, test_df, prefix=\"\"):\n",
    "    figure_output =  pathlib.Path(\"figures\")\n",
    "    figure_output.mkdir(parents=True, exist_ok=True)\n",
    "    for column in time_sensitive_features:\n",
    "        fig = plt.figure()\n",
    "        plot = sns.violinplot(data={\"train\":train_df[column], \"cv\":cv_df[column], \"test\":test_df[column]})\n",
    "        #plot = sns.violinplot(data={\"train\":train_df[column]})\n",
    "        plot.set(title=f\"{prefix}_{column}\")\n",
    "        plot.get_figure().savefig(figure_output/f\"{prefix}_{column}.png\") \n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_df_clean, cv_df_clean, test_df_clean, \"clean\")\n",
    "plot(train_df, cv_df, test_df, \"raw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow_vs_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
